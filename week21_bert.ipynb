{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678e378e-5eb4-4b4f-9dc7-44f27fc02510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\riset\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4ee95b-7460-4d88-8336-c72456526cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>No</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_fdist</th>\n",
       "      <th>normalized</th>\n",
       "      <th>tokens_stemmed</th>\n",
       "      <th>joined_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>baru saja kenalan sama orang baru di kafe dan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['baru', 'saja', 'kenalan', 'sama', 'orang', '...</td>\n",
       "      <td>['kenalan', 'orang', 'kafe', 'seru', 'banget',...</td>\n",
       "      <td>['baru', 'saja', 'kenalan', 'sama', 'orang', '...</td>\n",
       "      <td>['baru', 'saja', 'kenal', 'sama', 'orang', 'ba...</td>\n",
       "      <td>baru saja kenal sama orang baru di kafe dan se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>gak bisa diem di rumah nih pengen banget jalan...</td>\n",
       "      <td>1</td>\n",
       "      <td>['gak', 'bisa', 'diem', 'di', 'rumah', 'nih', ...</td>\n",
       "      <td>['diem', 'rumah', 'pengen', 'banget', 'jalanja...</td>\n",
       "      <td>['tidak', 'bisa', 'diem', 'di', 'rumah', 'nih'...</td>\n",
       "      <td>['tidak', 'bisa', 'diem', 'di', 'rumah', 'nih'...</td>\n",
       "      <td>tidak bisa diem di rumah nih ken banget jalanj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>ngecash energi dengan ketemu orang kantor nih ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['ngecash', 'energi', 'dengan', 'ketemu', 'ora...</td>\n",
       "      <td>['ngecash', 'energi', 'ketemu', 'orang', 'kant...</td>\n",
       "      <td>['ngecash', 'energi', 'dengan', 'ketemu', 'ora...</td>\n",
       "      <td>['ngecash', 'energi', 'dengan', 'ketemu', 'ora...</td>\n",
       "      <td>ngecash energi dengan ketemu orang kantor nih ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hari ini sibuk banget tapi seneng karena bisa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hari', 'ini', 'sibuk', 'banget', 'tapi', 'se...</td>\n",
       "      <td>['sibuk', 'banget', 'seneng', 'ketemu', 'orang...</td>\n",
       "      <td>['hari', 'ini', 'sibuk', 'banget', 'tetapi', '...</td>\n",
       "      <td>['hari', 'ini', 'sibuk', 'banget', 'tetapi', '...</td>\n",
       "      <td>hari ini sibuk banget tetapi neng karena bisa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>pesta di rumah tetangga bikin hari jadi gaul b...</td>\n",
       "      <td>1</td>\n",
       "      <td>['pesta', 'di', 'rumah', 'tetangga', 'bikin', ...</td>\n",
       "      <td>['pesta', 'rumah', 'tetangga', 'gaul', 'banget...</td>\n",
       "      <td>['pesta', 'di', 'rumah', 'tetangga', 'bikin', ...</td>\n",
       "      <td>['pesta', 'di', 'rumah', 'tetangga', 'bikin', ...</td>\n",
       "      <td>pesta di rumah tetangga bikin hari jadi gaul b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  No                                               Text  Label  \\\n",
       "0           0   1  baru saja kenalan sama orang baru di kafe dan ...      1   \n",
       "1           1   2  gak bisa diem di rumah nih pengen banget jalan...      1   \n",
       "2           2   3  ngecash energi dengan ketemu orang kantor nih ...      1   \n",
       "3           3   4  hari ini sibuk banget tapi seneng karena bisa ...      1   \n",
       "4           4   5  pesta di rumah tetangga bikin hari jadi gaul b...      1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['baru', 'saja', 'kenalan', 'sama', 'orang', '...   \n",
       "1  ['gak', 'bisa', 'diem', 'di', 'rumah', 'nih', ...   \n",
       "2  ['ngecash', 'energi', 'dengan', 'ketemu', 'ora...   \n",
       "3  ['hari', 'ini', 'sibuk', 'banget', 'tapi', 'se...   \n",
       "4  ['pesta', 'di', 'rumah', 'tetangga', 'bikin', ...   \n",
       "\n",
       "                                        tokens_fdist  \\\n",
       "0  ['kenalan', 'orang', 'kafe', 'seru', 'banget',...   \n",
       "1  ['diem', 'rumah', 'pengen', 'banget', 'jalanja...   \n",
       "2  ['ngecash', 'energi', 'ketemu', 'orang', 'kant...   \n",
       "3  ['sibuk', 'banget', 'seneng', 'ketemu', 'orang...   \n",
       "4  ['pesta', 'rumah', 'tetangga', 'gaul', 'banget...   \n",
       "\n",
       "                                          normalized  \\\n",
       "0  ['baru', 'saja', 'kenalan', 'sama', 'orang', '...   \n",
       "1  ['tidak', 'bisa', 'diem', 'di', 'rumah', 'nih'...   \n",
       "2  ['ngecash', 'energi', 'dengan', 'ketemu', 'ora...   \n",
       "3  ['hari', 'ini', 'sibuk', 'banget', 'tetapi', '...   \n",
       "4  ['pesta', 'di', 'rumah', 'tetangga', 'bikin', ...   \n",
       "\n",
       "                                      tokens_stemmed  \\\n",
       "0  ['baru', 'saja', 'kenal', 'sama', 'orang', 'ba...   \n",
       "1  ['tidak', 'bisa', 'diem', 'di', 'rumah', 'nih'...   \n",
       "2  ['ngecash', 'energi', 'dengan', 'ketemu', 'ora...   \n",
       "3  ['hari', 'ini', 'sibuk', 'banget', 'tetapi', '...   \n",
       "4  ['pesta', 'di', 'rumah', 'tetangga', 'bikin', ...   \n",
       "\n",
       "                                       joined_tokens  \n",
       "0  baru saja kenal sama orang baru di kafe dan se...  \n",
       "1  tidak bisa diem di rumah nih ken banget jalanj...  \n",
       "2  ngecash energi dengan ketemu orang kantor nih ...  \n",
       "3  hari ini sibuk banget tetapi neng karena bisa ...  \n",
       "4  pesta di rumah tetangga bikin hari jadi gaul b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = 'dataset_preprocess_fix.csv'  # Ganti dengan path dataset Anda\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3fdc17-67e3-403a-b93e-64795eb54815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06cc008-285e-4fe0-9ff5-7d99f3392f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['joined_tokens'], df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007d89d5-deed-4d45-880d-8d1f60c32283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\riset\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing the data\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3278c3ff-1ead-4dbf-a258-f9ba6114ca05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the tokenized data into a TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(1000).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba5241d-12ed-41a1-9bcc-4798359a2075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\riset\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels\n",
    "num_labels = len(df['Label'].unique())\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46615dea-3909-44bc-92bc-7d632b092c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b77a531-015a-4533-8d99-4a420dd03d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "81/81 [==============================] - 714s 8s/step - loss: 2.1032 - accuracy: 0.2036 - val_loss: 1.6094 - val_accuracy: 0.1605\n",
      "Epoch 2/10\n",
      "81/81 [==============================] - 689s 8s/step - loss: 1.6094 - accuracy: 0.2020 - val_loss: 1.6094 - val_accuracy: 0.1605\n",
      "Epoch 3/10\n",
      "81/81 [==============================] - 570s 7s/step - loss: 1.6094 - accuracy: 0.1935 - val_loss: 1.6094 - val_accuracy: 0.1605\n",
      "Epoch 4/10\n",
      "81/81 [==============================] - 563s 7s/step - loss: 1.6094 - accuracy: 0.2090 - val_loss: 1.6094 - val_accuracy: 0.1605\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=test_dataset,\n",
    "                    epochs=10,\n",
    "                    callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "507f03d7986614021c19596fe443dba4a216e1222855bc22301586b8fd406177"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
